{"cells":[{"cell_type":"markdown","metadata":{"id":"2cHqo6b1_Bzk"},"source":["# Implement a Neural Network\n","\n","This notebook contains testing code to help you develop a neural network by implementing the forward pass and backpropagation algorithm in the `models/neural_net.py` file. \n","\n","You will implement your network in the class `NeuralNetwork` inside the file `models/neural_net.py` to represent instances of the network. The network parameters are stored in the instance variable `self.params` where keys are string parameter names and values are numpy arrays."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"nTt_CiWh_Bzm"},"outputs":[],"source":["import numpy as np\n","\n","from models.neural_net import NeuralNetwork\n","\n","# For auto-reloading external modules\n","# See http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2\n","\n","def rel_error(x, y):\n","    \"\"\"Returns relative error\"\"\"\n","    \n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"]},{"cell_type":"markdown","metadata":{"id":"b5X9DO-5_Bzn"},"source":["The cell below initializes a toy dataset and corresponding model which will allow you to check your forward and backward pass by using a numeric gradient check. Note that we set a random seed for repeatable experiments."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"358jAXcc_Bzn"},"outputs":[],"source":["input_size = 2\n","hidden_size = 10\n","num_classes = 3\n","num_inputs = 5\n","optimizer = 'SGD'\n","\n","\n","def init_toy_model(num_layers):\n","    \"\"\"Initializes a toy model\"\"\"\n","    np.random.seed(0)\n","    hidden_sizes = [hidden_size] * (num_layers - 1)\n","    return NeuralNetwork(input_size, hidden_sizes, num_classes, num_layers, 'Adam', batch_size=5)\n","\n","def init_toy_data():\n","    \"\"\"Initializes a toy dataset\"\"\"\n","    np.random.seed(0)\n","    X = np.random.randn(num_inputs, input_size)\n","    y = np.random.randn(num_inputs, num_classes)\n","    return X, y\n"]},{"cell_type":"markdown","metadata":{"id":"zh_v9biP_Bzn"},"source":["# Implement forward and backward pass\n","\n","The first thing you will do is implement the forward pass of your neural network. The forward pass should be implemented in the `forward` function. You can use helper functions like `linear`, `relu`, and `softmax` to help organize your code.\n","\n","Next, you will implement the backward pass using the backpropagation algorithm. Backpropagation will compute the gradient of the loss with respect to the model parameters `W1`, `b1`, ... etc. Use a softmax fuction with cross entropy loss for loss calcuation. Fill in the code blocks in `NeuralNetwork.backward`. "]},{"cell_type":"markdown","metadata":{"id":"GjAwpT2z_Bzo"},"source":["# Gradient  check\n","\n","If you have implemented your forward pass through the network correctly, you can use the following cell to debug your backward pass with a numeric gradient check. If your backward pass has been implemented correctly, the max relative error between your analytic solution and the numeric solution should be around 1e-7 or less for all parameters.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"UZM47qUP_Bzo"},"outputs":[{"name":"stdout","output_type":"stream","text":["-----------\n","W1 max relative error: 1.192445e-09\n","b1 max relative error: 4.338837e-10\n","W2 max relative error: 4.023402e-08\n","b2 max relative error: 6.769678e-10\n","-----------\n","W1 max relative error: 1.124219e-08\n","b1 max relative error: 3.926770e-09\n","W2 max relative error: 2.494195e-08\n","b2 max relative error: 8.426723e-10\n","W3 max relative error: 7.354337e-10\n","b3 max relative error: 7.659362e-11\n"]}],"source":["from copy import deepcopy\n","\n","from utils.gradient_check import eval_numerical_gradient\n","\n","X, y = init_toy_data()\n","\n","\n","def f(W):\n","    net.forward(X)\n","    return net.backward(y)\n","\n","for num in [2, 3]:\n","    \n","    net = init_toy_model(num)\n","    net.forward(X)\n","    net.backward(y)\n","    print('-----------')\n","    gradients = deepcopy(net.gradients)\n","\n","    for param_name in net.params:\n","        param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n","        print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, gradients[param_name])))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"genrl","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"2485f12ec1575461afa18dd60b59b68c7b3bcc45ac6c1bf3c3eeab7c5fa09b69"}}},"nbformat":4,"nbformat_minor":0}
